> 1、实现批梯度下降算法
> - 只需要写一个函数，函数的定义为batch_gradient_descent(X_train, y_train, init_theta, eta=0.01, n_iters=1e4, epsilon=1e-8)
>  - X_train为训练集，y_train为训练集标签，init_theta为初始的theta值，eta为步长（默认为0.01），n_iters为最高迭代次数(默认为10000)，epsilon为允许的误差范围（默认为1e-8）。
> - 输出为最终的theta值。
> - 需要进行必要的断言，使算法更加健壮。
