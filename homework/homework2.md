> 1、实现批梯度下降和随机梯度下降算法
> - 只需要写一个函数，函数的定义为batch_gradient_descent(X_train, y_train, eta=0.01, n_iters=1e4, epsilon=1e-8)
>  - X_train为训练集，y_train为训练集标签，eta为步长（默认为0.01），n_iters为最高迭代次数(默认为10000)，epsilon为允许的误差范围（默认为1e-8）
